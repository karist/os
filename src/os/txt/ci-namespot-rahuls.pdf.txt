Computational_JJ Intelligence_NNP ,_, Volume_NN X_NNP ,_, Number_NNP Y_NNP ,_, 2000_CD APPLYING_NNP MACHINE_NNP LEARNING_NNP FOR_IN HIGH_NNP PERFORMANCE_NNP NAMED-ENTITY_NNP EXTRACTION_NNP SHUMEET_NNP BALUJA_NNP VIBHU_NNP O._NNP MITTAL_NNP RAHUL_NNP SUKTHANKAR_NNP Just_RB Research_NNP ,_, 4616_CD Henry_NNP Street_NNP ,_, Pittsburgh_NNP ,_, PA_NNP 15213_CD &_CC School_NNP of_IN Computer_NNP Science_NNP ,_, Carnegie_NNP Mellon_NNP University_NNP ,_, Pittsburgh_NNP ,_, PA_NNP 15213_CD This_DT paper_NN describes_VBZ a_DT machine_NN learning_VBG approach_NN to_TO building_VBG an_DT efficient_JJ ,_, accurate_JJ and_CC fast_JJ name_NN spotting_VBG system_NN ._. Finding_VBG names_NNS in_IN free_JJ text_NN is_VBZ an_DT important_JJ task_NN in_IN many_JJ text-based_JJ applications_NNS ._. Most_RBS previous_JJ approaches_NNS were_VBD based_VBN on_IN hand-crafted_JJ modules_NNS encoding_VBG language_NN and_CC genre-specific_JJ knowledge_NN ._. These_DT approaches_NNS had_VBD at_IN least_JJS two_CD shortcomings_NNS :_: they_PRP required_VBD large_JJ amounts_NNS of_IN time_NN and_CC expertise_NN to_TO develop_VB ,_, and_CC were_VBD not_RB easily_RB portable_JJ to_TO new_JJ languages_NNS and_CC genres_NNS ._. This_DT paper_NN describes_VBZ an_DT extensible_JJ system_NN which_WDT automatically_RB combines_VBZ weak_JJ evidence_NN from_IN different_JJ ,_, easily_RB available_JJ sources_NNS :_: part-of-speech_NN tags_NNS ,_, dictionaries_NNS ,_, and_CC surface-level_JJ syntactic_JJ information_NN such_JJ as_IN capitalization_NN and_CC punctuation_NN ._. Individually_RB ,_, each_DT piece_NN of_IN evidence_NN is_VBZ insufficient_JJ for_IN robust_JJ name_NN detection_NN ._. However_RB ,_, the_DT combination_NN of_IN evidence_NN ,_, through_IN standard_JJ machine_NN learning_NN techniques_NNS ,_, yields_NNS a_DT system_NN that_WDT achieves_VBZ performance_NN equivalent_NN to_TO the_DT best_JJS existing_VBG hand-crafted_JJ approaches_NNS ._. 1_CD ._. INTRODUCTION_NN Spotting_VBG named-entities_NNS in_IN text_NN can_MD be_VB an_DT important_JJ component_NN of_IN tasks_NNS such_JJ as_IN information_NN extraction_NN and_CC retrieval_NN ,1_IN restoration_NN of_IN capitalization_NN in_IN single-case_JJ text_NN ,_, and_CC spelling-correction_NN —_NN to_TO avoid_VB accidentally_RB “correcting_VBG ”_JJ names_NNS that_WDT are_VBP mistaken_VBN as_IN misspelled_JJ words_NNS ._. Most_JJS previous_JJ research_NN efforts_NNS in_IN building_VBG named-entity_NN systems_NNS relied_VBN on_IN carefully_RB hand-crafted_JJ rules_NNS ._. There_EX are_VBP two_CD impediments_NNS to_TO this_DT approach_NN ._. First_RB ,_, most_JJS of_IN the_DT typical_JJ hand-coded_JJ systems_NNS are_VBP optimized_JJ to_TO work_VB well_RB in_IN very_RB specific_JJ situations_NNS ;_: however_RB ,_, both_DT language_NN and_CC users_NNS ’_VBN needs_NNS are_VBP likely_JJ to_TO evolve_VB over_IN time_NN ._. Second_RB ,_, it_PRP is_VBZ not_RB clear_JJ how_WRB such_JJ systems_NNS could_MD be_VB easily_RB modified_VBN to_TO reflect_VB less_RBR well_RB behaved_VBN texts_NNS –_RB ones_NNS that_WDT may_MD have_VB misspelled_VBN words_NNS ,_, missing_VBG case_NN information_NN ,_, or_CC foreign_JJ words/phrases_NNS [12]_VBP ._. This_DT paper_NN presents_VBZ a_DT system_NN for_IN named-entity_NN extraction_NN that_WDT is_VBZ automatically_RB trained_VBN to_TO rec-_VB ognize_NN named-entities_NNS using_VBG statistical_JJ evidence_NN from_IN a_DT training_NN set_VBN ._. This_DT approach_NN has_VBZ several_JJ ad-_NNS vantages_NNS :_: first_JJ ,_, it_PRP eliminates_VBZ the_DT need_NN for_IN expert_NN language-specific_JJ linguistic_JJ knowledge_NN ._. To_TO train_VB the_DT automated_JJ system_NN ,_, users_NNS only_RB need_VBP to_TO tag_VB items_NNS that_WDT interest_NN them_PRP ._. If_IN the_DT users_NNS ’_VBN needs_VBZ change_NN ,_, the_DT system_NN can_MD re-learn_VB from_IN new_JJ data_NNS quickly_RB ._. Second_JJ ,_, system_NN performance_NN can_MD be_VB improved_VBN by_IN increas_NNS -_: ing_NN the_DT amount_NN of_IN training_NN data_NNS without_IN requiring_VBG extra_JJ expert_NN knowledge_NN ._. Third_JJ ,_, if_IN new_JJ knowledge_NN sources_NNS become_VBP available_JJ ,_, they_PRP can_MD easily_RB be_VB integrated_VBN into_IN the_DT system_NN as_IN additional_JJ evidence_NN ._. 2._VBD BACKGROUND_NNP AND_CC PREVIOUS_JJ WORK_NN Previous_JJ work_NN in_IN this_DT area_NN has_VBZ largely_RB taken_VBN place_NN in_IN the_DT context_NN of_IN the_DT Message_NN Understanding_NN Conferences_NNS (_-LRB- MUCs_NNS )_-RRB- [9]_VBP ._. For_IN MUC_NNP ,_, the_DT problem_NN of_IN finding_VBG named-entities_NNS was_VBD divided_VBN into_IN three_CD sub-problems_NNS ENAMEX_NNP ,_, finding_VBG entity_NN names_NNS (organizations_NNS ,_, persons_NNS and_CC locations_NNS )_-RRB- ;_: TIMEX_NNP ,_, finding_VBG temporal_JJ expressions_NNS (_-LRB- dates_NNS and_CC times_NNS )_-RRB- ;_: and_CC NUMEX_NNP ,_, finding_VBG numerical_JJ quantities_NNS (_-LRB- monetary_JJ values_NNS ,_, percentages_NNS ,_, etc._FW )_-RRB- ._. However_RB ,_, as_IN discussed_VBN by_IN Palmer_NNP and_CC Day_NNP [12]_NNP ,_, the_DT latter_JJ two_CD tasks_NNS are_VBP far_RB simpler_JJR than_IN the_DT first_JJ ._. (_-LRB- For_IN instance_NN ,_, they_PRP found_VBD that_IN nearly_RB all_DT NUMEX_NNP phrases_NNS could_MD be_VB identified_VBN reliably_RB 1A_DT study_NN on_IN IR_NNP in_IN a_DT legal_JJ domain_NN found_VBD a_DT 20_CD %_NN improvement_NN in_IN precision_NN when_WRB users_NNS could_MD specifically_RB search_VB for_IN names_NNS [18]_IN ._. c©_'' 2000_CD Blackwell_NNPS Publishers_NNPS ,_, 238_CD Main_NNP Street_NNP ,_, Cambridge_NNP ,_, MA_NNP 02142_CD ,_, USA_NNP ,_, and_CC 108_CD Cowley_NNP Road_NNP ,_, Oxford_NNP ,_, OX4_NNP 1JF_NNP ,_, UK._NNP 2_CD COMPUTATIONAL_NNP INTELLIGENCE_NN with_IN a_DT very_RB small_JJ number_NN of_IN patterns_NNS ._. )_-RRB- Since_IN ENAMEX_NNP has_VBZ been_VBN suggested_VBN to_TO be_VB the_DT most_RBS difficult_JJ of_IN the_DT three_CD sub-tasks_NNS ,_, this_DT paper_NN concentrates_VBZ on_IN name_NN detection_NN ._. Early_RB work_NN on_IN name_NN detection_NN was_VBD based_VBN on_IN either_DT :_: (_-LRB- 1_CD )_-RRB- hand-crafted_JJ regular_JJ expressions_NNS [2_CC ;_: 3_CD ;_: 20]_CD ;_: (_-LRB- 2_CD )_-RRB- extensive_JJ resources_NNS such_JJ as_IN lists_NNS of_IN geographic_JJ locations_NNS ,_, people_NNS and_CC company_NN names_NNS ,_, etc_FW ._. [10]_'' ;_: or_CC (_-LRB- 3_LS )_-RRB- sophisticated_JJ linguistic_JJ approaches_NNS based_VBN on_IN parsing_NN [11_CD ;_: 10_CD ;_: 8]_CD ._. Such_JJ systems_NNS can_MD be_VB quite_RB expensive_JJ to_TO develop_VB and_CC maintain_VB ._. Systems_NNP that_WDT exploit_VBP machine_NN learning_NN are_VBP more_RBR closely_RB related_VBN to_TO our_PRP$ work_NN ._. NYMBLE_NNP [4]_NNP ,_, based_VBN on_IN a_DT Hidden_NNP Markov_NNP Model_NNP (_-LRB- HMM)_NNP [15]_NNP ,_, achieves_VBZ good_JJ performance_NN at_IN the_DT expense_NN of_IN large_JJ computational_JJ resources_NNS ._. ALEMBIC_NN [1_VBD ]_SYM performs_VBZ named-entity_NN extraction_NN by_IN learning_VBG rule_NN sequences_NNS ,_, but_CC uses_VBZ different_JJ feature_NN encodings_NNS and_CC does_VBZ not_RB parameterize_VB the_DT contribution_NN of_IN its_PRP$ various_JJ components_NNS ._. 3._'' KNOWLEDGE_NNP SOURCES_NNP USED_NNP To_TO determine_VB whether_IN a_DT token_NN is_VBZ a_DT name_NN ,_, our_PRP$ system_NN uses_VBZ weak_JJ evidence_NN from_IN a_DT number_NN of_IN sources_NNS ._. The_DT main_JJ consideration_NN in_IN deciding_VBG which_WDT information_NN sources_NNS to_TO use_VB was_VBD the_DT difficulty_NN associated_VBN with_IN creating_VBG and_CC maintaining_VBG such_JJ sources_NNS ._. A_DT secondary_JJ consideration_NN was_VBD to_TO keep_VB the_DT learned_JJ models_NNS as_IN small_JJ as_IN possible_JJ ._. The_DT knowledge_NN sources_NNS ,_, encoded_VBN as_IN a_DT set_NN of_IN 29_CD features_NNS ,_, are_VBP :_: Word_NNP Level_NNP Features_NNP :_: Language_VB -_: or_CC genre-specific_JJ cues_NNS can_MD sometimes_RB be_VB exploited_VBN to_TO provide_VB evidence_NN for_IN name_NN detection_NN (_-LRB- e_NN .g._NNP ,_, in_IN English_NNP ,_, names_NNS are_VBP often_RB capitalized_JJ )_-RRB- ._. The_DT following_VBG features_NNS are_VBP used_VBN in_IN encoding_VBG tokens_NNS and_CC the_DT system_NN learns_VBZ which_WDT of_IN these_DT correlate_JJ strongly_RB with_IN names_NNS :_: (_-LRB- 1_CD )_-RRB- all-uppercase_JJ ,_, (_-LRB- 2_CD )_-RRB- initial-caps_NN ,_, (_-LRB- 3_CD )_-RRB- all-numbers_NNS ,_, (_-LRB- 4_CD )_-RRB- alphanumeric_NN ,_, (_-LRB- 5_CD )_-RRB- single-char_NN ,_, (_-LRB- 6_CD )_-RRB- single-s_NN (_-LRB- if_IN the_DT token_NN is_VBZ the_DT character_NN “s”_NN )_-RRB- ,_, and_CC (_-LRB- 7_CD )_-RRB- single-i_FW (_-LRB- if_IN the_DT token_NN is_VBZ the_DT character_NN “I”_NN )_-RRB- ._. Individually_NNP ,_, none_NN of_IN the_DT local_JJ word-level_NN features_NNS are_VBP very_RB effective_JJ :_: the_DT strongest_JJS individual_JJ feature_NN is_VBZ all-caps_RB ;_: it_PRP flags_NNS 474_CD tokens_NNS in_IN the_DT training_NN set_NN (_-LRB- containing_VBG 50,416_CD tokens_NNS ,_, of_IN which_WDT 3,632_CD are_VBP names_NNS )_-RRB- ._. Of_IN these_DT ,_, 449_CD are_VBP actually_RB names_NNS ,_, the_DT rest_NN being_VBG non-name_JJ acronyms_NNS such_JJ as_IN “CEO”_NNP and_CC “PC”_NNP ,_, yielding_VBG an_DT F1-score2_JJ of_IN only_RB 21_CD (_-LRB- Precision_NN =_SYM 94_CD ;_: Recall_NNP =_SYM 12_CD )_-RRB- ._. The_DT feature_NN initial-caps_NNS is_VBZ similar_JJ ,_, flagging_VBG 5,650_CD words_NNS ,_, of_IN which_WDT 3,572_CD are_VBP names_NNS ,_, leading_VBG to_TO an_DT F1-score_NN of_IN 82_CD (_-LRB- Precision_NNP =_SYM 73_CD ;_: Recall_NNP =_SYM 94)_CD ._. Note_VB that_IN not_RB all_DT capitalized_JJ words_NNS are_VBP names_NNS ,_, and_CC that_IN not_RB all_DT names_NNS are_VBP capitalized_JJ (_-LRB- e_NN .g._NNP ,_, “van_JJ Gogh”_NNP )_-RRB- ._. For_IN English_NNP text_NN ,_, capitalization_NN ,_, in_IN conjunction_NN with_IN reliable_JJ end-of-sentence_NN boundary_NN detec-_, tion_NN ,_, is_VBZ a_DT good_JJ indicator_NN for_IN names_NNS ._. However_RB ,_, determining_VBG sentence_NN boundaries_NNS is_VBZ difficult_JJ since_IN common_JJ boundaries_NNS such_JJ as_IN periods_NNS ,_, question_NN -_: and_CC exclamation-marks_NNS can_MD occur_VB in_IN many_JJ different_JJ contexts_NNS [16_VBD ;_: 13]_CD ._. While_IN the_DT system_NN does_VBZ not_RB explicitly_RB contain_VB rules_NNS for_IN sentence_NN boundary_NN analysis_NN ,_, by_IN using_VBG contextual_JJ cues_NNS ,_, it_PRP can_MD account_VB for_IN many_JJ sentence_NN boundaries_NNS ._. Dictionary_NNP Look-Up_NNP :_: Another_DT weak_JJ heuristic_JJ for_IN determining_VBG whether_IN a_DT particular_JJ token_NN is_VBZ a_DT name_NN is_VBZ to_TO check_VB whether_IN it_PRP can_MD be_VB found_VBN in_IN a_DT dictionary_NN ._. Since_IN many_JJ names_NNS are_VBP not_RB valid_JJ English_JJ words_NNS ,_, this_DT resource_NN can_MD identify_VB some_DT potential_JJ names_NNS ._. The_DT dictionary_NN used_VBD in_IN our_PRP$ experiments_NNS was_VBD the_DT standard_JJ spelling_NN dictionary_NN available_JJ on_IN most_JJS UNIX_NNP systems_NNS ;_: it_PRP contained_VBD 45,402_CD words_NNS ,_, of_IN which_WDT 6,784_CD were_VBD capitalized_VBN ,_, and_CC were_VBD discarded_VBN as_IN names_NNS ._. The_DT remaining_VBG 38,618_CD tokens_NNS contained_VBD multiple_JJ mor-_NNS phological_JJ variants_NNS of_IN the_DT same_JJ word_NN (_-LRB- further_RBR decreasing_VBG the_DT number_NN of_IN unique_JJ root_NN forms_NNS )_-RRB- ._. Finally_RB ,_, since_IN a_DT number_NN of_IN English_JJ names_NNS are_VBP also_RB part_NN of_IN the_DT regular_JJ vocabulary_NN (_-LRB- e_NN .g._NNP ,_, “mark”_NNP ,_, “baker_NN ”_NN and_CC 2Performance_VB on_IN the_DT name_NN detection_NN task_NN is_VBZ typically_RB measured_VBN by_IN the_DT Fβ_NNP score_NN [19]_NN ,_, which_WDT is_VBZ a_DT combination_NN of_IN the_DT Precision_NNP (_-LRB- P_NNP )_-RRB- and_CC Recall_NNP (_-LRB- R_NN )_-RRB- measures_VBZ used_VBN in_IN IR_NNP ._. The_DT Fβ_NNP score_NN is_VBZ defined_VBN to_TO be_VB :_: (_-LRB- β_NN 2+1_CD )∗R∗P_NNP β2∗P+R_NNP ,_, where_WRB β_NN is_VBZ usually_RB set_VBN to_TO 1_CD ._. Studies_NNPS have_VBP shown_VBN that_IN ,_, on_IN average_NN ,_, the_DT F1_JJ score_NN for_IN manually_RB finding_VBG names_NNS in_IN text_NN is_VBZ approximately_RB 96_CD [9]_NN ._. In_IN comparison_NN ,_, the_DT F1_JJ scores_NNS for_IN many_JJ manually_NN crafted_VBN systems_NNS are_VBP often_RB between_IN 90_CD and_CC 92_CD [10_CD ;_: 8_CD ;_: 5_CD ;_: 17]_CD ._. APPLYING_NNP MACHINE_NNP LEARNING_NNP FOR_IN HIGH_NNP PERFORMANCE_NNP NAMED-ENTITY_NNP EXTRACTION_NNP 3_CD “stone”_NN )_-RRB- ,_, name_NN detection_NN using_VBG only_JJ evidence_NN from_IN the_DT dictionary_NN is_VBZ not_RB very_RB reliable_JJ :_: the_DT F1-score_NN for_IN the_DT dictionary_NN module_NN alone_RB on_IN our_PRP$ training_NN set_NN was_VBD only_RB 64_CD ._. Part-of-Speech_NNP Tagger_NNP :_: Part-of-Speech_NNP (_-LRB- POS_NNS )_-RRB- tags_NNS can_MD be_VB used_VBN by_IN other_JJ modules_NNS to_TO reason_NN about_IN the_DT roles_NNS and_CC relative_JJ importance_NN of_IN words/tokens_NNS in_IN various_JJ contexts_NNS ._. In_IN this_DT system_NN ,_, we_PRP used_VBD the_DT Brill_NNP tagger_NN for_IN POS_NNP tagging3_CD ._. Brill_NNP reports_VBZ approximately_RB 97_CD %_NN overall_JJ accuracy_NN for_IN words_NNS in_IN the_DT WSJ_NNP corpus_NN for_IN the_DT tagger_NN [6_VBD ;_: 7]_CD ._. Its_PRP$ performance_NN is_VBZ lower_JJR on_IN the_DT named-entity_NN task_NN :_: on_IN our_PRP$ training_NN data_NNS ,_, the_DT tagger_NN obtained_VBD an_DT F1-score_JJ of_IN 83_CD (_-LRB- P_NNP =_SYM 81_CD ,_, R_NN =_SYM 86_CD )_-RRB- ;_: consistent_JJ results_NNS are_VBP reported_VBN in_IN [1]_NNP ._. The_DT following_VBG POS_NNP tags_NNS were_VBD used_VBN as_IN features_NNS by_IN the_DT machine_NN learning_VBG component_NN of_IN our_PRP$ system_NN :_: (_-LRB- 1_CD )_-RRB- determiner_NN ,_, (_-LRB- 2_CD )_-RRB- foreign-word_NN ,_, (_-LRB- if_IN the_DT token_NN is_VBZ one_CD that_IN the_DT tagger_NN has_VBZ not_RB seen_VBN )_-RRB- ,_, (_-LRB- 3_CD )_-RRB- preposition_NN ,_, (_-LRB- 4_CD )_-RRB- adjective_NN ,_, (_-LRB- 5_CD )_-RRB- noun_NN ,_, (_-LRB- 6_CD )_-RRB- proper-noun_NN ,_, (_-LRB- 7_CD )_-RRB- personal-pronoun_NN ,_, (_-LRB- 8_CD )_-RRB- possessive-pronoun_NN ,_, (_-LRB- 9_CD )_-RRB- verb_NN ,_, (_-LRB- 10_CD )_-RRB- WH-pronoun_NN (_-LRB- which_WDT ,_, what_WP ,_, etc._FW )_-RRB- ,_, (_-LRB- 11_CD )_-RRB- unknown-POS._JJ Punctuation_NN :_: Robust_NNP name_NN detection_NN probably_RB requires_VBZ that_IN the_DT system_NN capture_NN contextual_JJ syntactic_NN information_NN ._. (_-LRB- At_IN the_DT very_RB least_JJS ,_, to_TO disambiguate_VB capitalization_NN cues_NNS due_JJ to_TO sentence_NN boundaries_NNS ._. )_-RRB- The_DT system_NN learns_VBZ syntactic_JJ patterns_NNS that_WDT may_MD indicate_VB named_VBN entities_NNS ._. Section_NN 5_CD discusses_NNS the_DT effects_NNS of_IN varying_VBG the_DT size_NN of_IN this_DT contextual_JJ window_NN ._. The_DT following_VBG punctuation_NN characters_NNS are_VBP encoded_VBN as_IN features_NNS :_: (_-LRB- 1_CD )_-RRB- comma_NN ;_: (_-LRB- 2_CD )_-RRB- period_NN ;_: (_-LRB- 3_CD )_-RRB- exclamation_NN mark_NN ;_: (_-LRB- 4_CD )_-RRB- question_NN mark_NN ;_: (_-LRB- 5_CD )_-RRB- semi-colon_NN ;_: (_-LRB- 6_CD )_-RRB- colon_NN ;_: (_-LRB- 7_CD )_-RRB- plus_CC or_CC minus_CC sign_NN ;_: (_-LRB- 8_CD )_-RRB- apostrophe_NN ;_: (_-LRB- 9_CD )_-RRB- left_VBD parenthesis_NN ;_: (_-LRB- 10_CD )_-RRB- right_JJ parenthesis_NN ._. 4_CD ._. SYSTEM_NNP ARCHITECTURE_NNP The_DT name_NN spotting_VBG system_NN consists_VBZ of_IN two_CD components_NNS :_: a_DT tokenizer_NN and_CC a_DT classifier_NN ._. The_DT tokenizer_NN converts_VBZ text_NN into_IN a_DT set_NN of_IN features_NNS based_VBN on_IN the_DT knowledge_NN sources_NNS presented_VBN in_IN the_DT previous_JJ section_NN ._. These_DT tokens_NNS are_VBP used_VBN by_IN the_DT classifier_NN ,_, which_WDT is_VBZ a_DT decision_NN tree_NN constructed_VBN from_IN the_DT training_NN data_NNS based_VBN on_IN information_NN theory_NN (_-LRB- C4.5_JJ [14]_NN )_-RRB- ._. The_DT tokenizer_NN reads_VBZ input_NN text_NN and_CC creates_VBZ tokens_NNS consisting_VBG of_IN either_DT words_NNS or_CC selected_VBN punctua_NN -_: tion_NN marks_NNS ._. Each_DT token_NN is_VBZ encoded_VBN by_IN a_DT set_NN of_IN 29_CD boolean_NN features_NNS (_-LRB- +1_CD or_CC -1_NN )_-RRB- indicating_VBG the_DT presence_NN or_CC absence_NN of_IN that_DT feature_NN ._. Features_NNS that_WDT are_VBP not_RB used_VBN in_IN a_DT particular_JJ experiment_NN are_VBP given_VBN a_DT zero_NN value_NN ._. The_DT classifier_NN combines_VBZ the_DT various_JJ sources_NNS of_IN weak_JJ evidence_NN about_IN the_DT candidate_NN token_NN and_CC its_PRP$ context_NN ._. No_DT feature_NN ,_, by_IN itself_PRP ,_, is_VBZ sufficient_JJ for_IN robust_JJ classification_NN ._. The_DT goal_NN of_IN the_DT classifier_NN is_VBZ to_TO automatically_RB combine_VB all_DT of_IN the_DT evidence_NN to_TO determine_VB whether_IN the_DT candidate_NN token_NN is_VBZ a_DT name_NN ._. The_DT output_NN ,_, or_CC target_NN variable_JJ ,_, is_VBZ the_DT manually-coded_JJ label_NN identifying_VBG whether_IN the_DT token_NN is_VBZ a_DT name_NN ._. Since_IN the_DT classifier_NN does_VBZ not_RB explicitly_RB model_VB syntactic_JJ patterns_NNS in_IN the_DT text_NN ,_, the_DT decision_NN tree_NN learner_NN must_MD induce_VB a_DT large_JJ number_NN of_IN syntactic_JJ patterns_NNS to_TO account_VB for_IN varying_VBG numbers_NNS of_IN tokens_NNS that_WDT can_MD appear_VB in_IN a_DT particular_JJ syntactic_JJ role_NN ._. One_CD approach_NN to_TO dealing_VBG with_IN this_DT problem_NN is_VBZ to_TO collapse_VB adjacent_JJ syntactic_NN tokens_NNS from_IN the_DT same_JJ category_NN into_IN a_DT single_JJ token_JJ ._. This_DT compression_NN permits_VBZ the_DT system_NN to_TO learn_VB fewer_JJR patterns_NNS in_IN the_DT available_JJ data_NNS while_IN maintaining_VBG (_-LRB- or_CC perhaps_RB improving_VBG )_-RRB- accuracy_NN ._. Although_IN computationally_RB more_RBR expensive_JJ ,_, as_RB will_MD be_VB shown_VBN in_IN Section_NN 5_CD ,_, our_PRP$ experiments_NNS reveal_VBP an_DT improvement_NN in_IN performance_NN with_IN a_DT limited_JJ version_NN of_IN this_DT method_NN ._. 3Version_CD 1.1_CD ,_, with_IN 148_CD lexical_JJ rules_NNS and_CC 283_JJ contextual_JJ rules_NNS ,_, trained_VBN on_IN a_DT Wall_NNP Street_NNP Journal_NNP (_-LRB- WSJ)_NNP corpus_FW from_IN the_DT Linguistic_NNP Data_NNP Consortium_NNP with_IN a_DT lexicon_NN of_IN 93,696_CD words_NNS ._. 4_CD COMPUTATIONAL_NNP INTELLIGENCE_NNP 5_CD ._. EXPERIMENTAL_NN RESULTS_NNS Our_PRP$ experiments_NNS were_VBD conducted_VBN using_VBG a_DT training_NN set_NN of_IN 100_CD randomly-selected_JJ Reuters_NNS news_NN articles_NNS ,_, containing_VBG 50,416_CD tokens_NNS ,_, of_IN which_WDT 44,013_CD were_VBD words_NNS (_-LRB- the_DT rest_NN were_VBD punctuation_NN )_-RRB- ._. The_DT training_NN set_VBD included_VBN 3,632_CD names_NNS ,_, 1552_CD of_IN which_WDT were_VBD distinct_JJ ._. The_DT results_NNS reported_VBD in_IN this_DT section_NN were_VBD obtained_VBN by_IN running_VBG the_DT system_NN on_IN a_DT test_NN set_NN of_IN 25_CD additional_JJ articles_NNS (_-LRB- these_DT articles_NNS were_VBD not_RB used_VBN for_IN training_NN )_-RRB- ._. These_DT test_NN articles_NNS contained_VBD a_DT total_NN of_IN 13,507_CD tokens_NNS ,_, of_IN which_WDT 11,811_CD were_VBD words_NNS ,_, and_CC 1048_CD were_VBD labeled_VBN by_IN the_DT coders_NNS as_IN names_NNS ._. Section_NN 3_CD discussed_VBN baseline_NN performance_NN for_IN each_DT of_IN the_DT individual_JJ modules_NNS ._. One_CD simple_NN “learn_JJ -_: ing_NN ”_NNS approach_NN would_MD be_VB for_IN the_DT system_NN to_TO construct_VB a_DT list_NN of_IN names_NNS encountered_VBN in_IN the_DT training_NN set_NN and_CC match_NN candidate_NN tokens_VBZ against_IN this_DT list_NN during_IN testing_VBG ._. However_RB ,_, as_IN pointed_VBN out_IN in_IN [12]_NN ,_, this_DT is_VBZ un-_RB likely_JJ to_TO significantly_RB help_VB in_IN name_NN detection_NN ._. Our_PRP$ observations_NNS confirmed_VBD this_DT hypothesis_NN :_: the_DT test_NN set_NN contained_VBN 1048_CD names_NNS (_-LRB- of_IN which_WDT 441_CD were_VBD unique_JJ )_-RRB- ._. Of_IN these_DT 1048_CD names_NNS ,_, only_RB 110_CD names_NNS had_VBD appeared_VBN in_IN the_DT training_NN set_VBN ;_: therefore_RB ,_, the_DT system_NN cannot_MD simply_RB rely_VB on_IN a_DT list_NN of_IN names_NNS built_VBN during_IN training_NN ._. The_DT experimental_JJ procedure_NN was_VBD as_IN follows_VBZ :_: (_-LRB- 1_CD )_-RRB- the_DT manually_RB labeled_VBN data_NNS was_VBD divided_VBN into_IN three_CD sets_NNS ,_, training_NN ,_, validation_NN ,_, and_CC testing_NN ;_: (_-LRB- 2_CD )_-RRB- the_DT training_NN set_NN was_VBD used_VBN for_IN inducing_VBG the_DT decision_NN tree_NN ;_: (_-LRB- 3_CD )_-RRB- the_DT validation_NN set_NN was_VBD used_VBN to_TO prevent_VB over-fitting_JJ of_IN the_DT data_NN ;_: (_-LRB- 4_CD )_-RRB- during_IN training_NN ,_, the_DT error_NN on_IN the_DT validation_NN set_NN was_VBD tracked_VBN ._. When_WRB this_DT error_NN started_VBD to_TO increase_VB due_JJ to_TO overfitting_VBG ,_, the_DT training_NN was_VBD halted_VBN and_CC the_DT classifier_NN was_VBD evaluated_VBN on_IN the_DT testing_NN set_VBN ._. To_TO ensure_VB that_DT idiosyncrasies_NNS in_IN any_DT data-set_NN splitting_NN did_VBD not_RB affect_VB our_PRP$ results_NNS ,_, repeated_VBN tests_NNS were_VBD employed_VBN to_TO accurately_RB estimate_VB the_DT system’s_NN performance_NN ._. Each_DT experiment_NN was_VBD repeated_VBN 5_CD times_NNS ,_, using_VBG different_JJ parts_NNS of_IN the_DT data-set_NN for_IN training_NN and_CC validation_NN ._. In_IN each_DT experiment_NN ,_, 80_CD articles_NNS were_VBD used_VBN for_IN training_NN ,_, and_CC 20_CD for_IN validation_NN ._. All_DT of_IN the_DT results_NNS presented_VBN are_VBP measured_VBN on_IN the_DT performance_NN of_IN the_DT network_NN on_IN an_DT entirely_RB separate_JJ testing_NN set_VBN of_IN 25_CD articles_NNS ._. The_DT first_JJ line_NN of_IN Table_NN 1_CD shows_VBZ the_DT performance_NN of_IN the_DT system_NN with_IN only_RB the_DT information_NN for_IN the_DT word_NN to_TO be_VB classified_VBN ._. The_DT second_JJ line_NN shows_VBZ the_DT performance_NN when_WRB given_VBN context_NN information_NN ;_: in_IN addition_NN to_TO the_DT 29_CD entry_NN feature_NN vector_NN for_IN the_DT word_NN to_TO be_VB classified_VBN ,_, the_DT decision_NN tree_NN is_VBZ also_RB given_VBN the_DT feature_NN vectors_NNS for_IN one_CD word_NN before_IN and_CC after_IN the_DT word_NN to_TO be_VB classified_VBN ._. The_DT third_JJ and_CC fourth_JJ lines_NNS show_VBP similar_JJ context_NN information_NN for_IN 2_CD and_CC 6_CD words_NNS before_IN and_CC after_IN the_DT word_NN to_TO be_VB classified_VBN ,_, respectively_RB ._. Note_VB that_IN the_DT context_NN is_VBZ taken_VBN without_IN regard_NN to_TO sentence_NN boundaries_VBZ .4_CD For_IN simplicity_NN ,_, the_DT number_NN of_IN words_NNS examined_VBN before_RB and_CC after_IN the_DT candidate_NN token_NN were_VBD kept_VBN the_DT same_JJ ;_: however_RB ,_, this_DT is_VBZ not_RB a_DT requirement_NN for_IN the_DT algorithm_NN ._. We_PRP also_RB examined_VBD the_DT performance_NN of_IN using_VBG single_JJ and_CC pair-wise_JJ combinations_NNS of_IN knowledge_NN sources_NNS ;_: the_DT results_NNS of_IN these_DT are_VBP shown_VBN in_IN Table_NN 2._TO Note_VB that_IN the_DT dictionary_NN features_NNS combined_VBN with_IN the_DT word-level_NN features_NNS perform_VBP almost_RB as_RB well_RB as_IN the_DT word-level_NN features_NNS combined_VBN with_IN the_DT part-_NNS of-speech_NN (_-LRB- POS_NNP )_-RRB- tags_NNS ._. However_RB ,_, the_DT dictionary_NN and_CC POS_NNP tags_NNS combination_NN does_VBZ not_RB perform_VB as_RB well_RB ._. This_DT suggests_VBZ that_IN the_DT word_NN level_NN features_NNS contain_VBP information_NN that_WDT is_VBZ not_RB contained_VBN in_IN either_DT of_IN the_DT other_JJ two_CD sources_NNS ._. There_EX are_VBP several_JJ methods_NNS for_IN understanding_VBG the_DT importance_NN of_IN individual_JJ features_NNS ._. The_DT first_JJ is_VBZ to_TO examine_VB the_DT rules_NNS that_WDT are_VBP encoded_VBN in_IN the_DT decision_NN tree_NN ._. Sample_NNP rules_NNS are_VBP shown_VBN in_IN Figure_NN 1_CD ._. A_DT second_JJ method_NN to_TO understand_VB each_DT feature’s_JJ potential_JJ contribution_NN to_TO the_DT classification_NN is_VBZ to_TO ex_FW -_: amine_IN the_DT weights_NNS for_IN each_DT of_IN the_DT features_NNS in_IN a_DT simple_JJ perceptron_NN ._. In_IN this_DT study_NN ,_, the_DT perceptron_NN is_VBZ trained_VBN independently_RB from_IN the_DT decision-tree_NN ,_, and_CC achieves_VBZ approximately_RB the_DT same_JJ errors_NNS rates_NNS ._. With_IN the_DT perceptron_NN ,_, a_DT positive_JJ weight_NN implies_VBZ that_IN the_DT respective_JJ feature_NN is_VBZ positively_RB correlated_VBN with_IN the_DT candidate_NN token_NN being_VBG a_DT name_NN ;_: negative_JJ weights_NNS are_VBP negatively_RB correlated_VBN in_IN the_DT same_JJ manner_NN ._. Figure_NN 2_CD depicts_VBZ the_DT weights_NNS in_IN a_DT trained_JJ perceptron_NN with_IN context_NN of_IN 0_CD and_CC 1_CD respectively_RB ._. Sev-_'' 4A_DT token_JJ ,_, such_JJ as_IN a_DT period_NN ,_, which_WDT may_MD indicate_VB a_DT sentence_NN boundary_NN ,_, is_VBZ part_NN of_IN the_DT context_NN ,_, and_CC thus_RB enables_VBZ the_DT system_NN to_TO eventually_RB learn_VB about_IN rules_NNS for_IN capitalizing_VBG the_DT first_JJ word_NN of_IN a_DT sentence_NN ._. APPLYING_NN MACHINE_NNP LEARNING_NNP FOR_IN HIGH_NNP PERFORMANCE_NNP NAMED-ENTITY_NNP EXTRACTION_NNP 5_CD KNOWLEDGE_NNP Context_NNP Accuracy_NNP SOURCES_NNP words(_NN 0–6_IN )_-RRB- Recall_NNP Prec._NNP F1_NNP ALL_NNP 0_CD 0.932_CD 0.899_CD 91.5_CD ALL_NNP 1_CD 0.941_CD 0.923_CD 93.2_CD ALL_NNP 2_CD 0.942_CD 0.931_CD 93.1_CD ALL_NNP 6_CD 0.937_CD 0.924_CD 93.0_CD TABLE_NNP 1_CD ._. Using_VBG All_DT Knowledge_NNP Sources_NNS and_CC the_DT Effects_NNS of_IN Context_NN ._. (_-LRB- Averaged_JJ over_IN 5_CD runs_NNS )_-RRB- Kl_NNP ._. #_# of_IN Accuracy_NNP Sources_NNS words_NNS Recall_NNP Prec._NNP F1_NNP P_NNP 0_CD 0.855_CD 0.814_CD 83.3_CD P_NNP 1_CD 0.857_CD 0.802_CD 82.8_CD P_NNP 2_CD 0.854_CD 0.818_CD 83.5_CD P_NNP 6_CD 0.862_CD 0.808_CD 83.7_NN D_NNP 0_CD 0.910_CD 0.483_CD 62.8_NN D_NNP 1_CD 0.915_CD 0.480_CD 62.9_IN D_NNP 2_CD 0.911_CD 0.489_CD 63.6_NN D_NNP 6_CD 0.918_CD 0.476_CD 62.6_CD WL_NNP 0_CD 0.983_CD 0.637_CD 77.3_CD WL_NNP 1_CD 0.981_CD 0.640_CD 77.4_CD WL_NNP 2_CD 0.981_CD 0.613_CD 75.4_CD WL_NNP 6_CD 0.980_CD 0.627_CD 76.4_CD (_-LRB- a_DT )_-RRB- Effects_NNS of_IN varying_VBG context_NN ._. Kl_NNP ._. #_# of_IN Accuracy_NNP sources_NNS words_NNS Recall_NNP Prec._NNP F1_NNP D_NNP &_CC P_NNP 0_CD 0.646_CD 0.831_CD 72.6_IN D_NNP &_CC P_NNP 1_CD 0.760_CD 0.780_CD 76.9_IN D_NNP &_CC P_NNP 2_CD 0.822_CD 0.774_CD 79.7_IN D_NNP &_CC P_NNP 6_CD 0.752_CD 0.777_CD 76.4_IN D_NNP &_CC WL_NNP 0_CD 0.639_CD 0.929_CD 75.7_IN D_NNP &_CC WL_NNP 1_CD 0.931_CD 0.910_CD 92.0_IN D_NNP &_CC WL_NNP 2_CD 0.949_CD 0.911_CD 93.9_IN D_NNP &_CC WL_NNP 6_CD 0.941_CD 0.912_CD 92.6_CD P_NNP &_CC WL_NNP 0_CD 0.906_CD 0.912_CD 90.8_CD P_NNP &_CC WL_NNP 1_CD 0.911_CD 0.901_CD 90.5_CD P_NNP &_CC WL_NNP 2_CD 0.932_CD 0.918_CD 92.4_CD P_NNP &_CC WL_NNP 6_CD 0.923_CD 0.883_CD 90.2_CD (_-LRB- b_NN )_-RRB- Effects_NNS of_IN pair-wise_NN combinations_NNS ._. TABLE_NN 2._VBD Performance_NNP of_IN individual_JJ and_CC combined_VBN knowledge_NN sources_NNS ,_, with_IN different_JJ amounts_NNS of_IN context_NN averaged_VBD over_IN 5_CD runs_NNS ._. P=Part_NN of_IN Speech_NNP ,_, D=Dictionary_NNP ,_, WL=Word_NNP Level_NNP Features_NNP ._. eral_JJ attributes_NNS of_IN these_DT figures_NNS should_MD be_VB noticed_VBN ._. First_RB ,_, in_IN the_DT zero_CD context_NN case_NN ,_, the_DT features_NNS most_RBS indicative_JJ of_IN names_NNS are_VBP those_DT for_IN capitalization_NN ._. As_IN discussed_VBN in_IN Section_NN 3_CD ,_, POS_NNP tagging_NN for_IN proper_JJ nouns_NNS is_VBZ not_RB very_RB reliable_JJ ._. This_DT is_VBZ reflected_VBN by_IN the_DT medium_JJ weight_NN given_VBN to_TO the_DT proper_JJ noun_NN tag_NN by_IN the_DT classifier_NN ._. It_PRP is_VBZ worth_JJ noting_VBG that_IN the_DT POS-tagger’s_JJ label_NN of_IN noun_NN and_CC adjective_NN are_VBP indicative_JJ of_IN proper_JJ names_NNS ._. This_DT suggests_VBZ that_IN the_DT proper-name_JJ detection_NN of_IN the_DT tagger_NN sometimes_RB misses_VBZ nouns_NNS and_CC adjectives_NNS which_WDT should_MD be_VB labeled_VBN as_IN names_NNS ._. The_DT most_RBS salient_JJ features_NNS against_IN proper_JJ names_NNS are_VBP whether_IN the_DT word_NN exists_VBZ in_IN the_DT dictionary_NN and_CC whether_IN it_PRP is_VBZ a_DT single_JJ character_NN ._. In_IN the_DT single-word_JJ context_NN models_NNS ,_, the_DT classifiers_NNS learned_VBD to_TO exploit_VB domain-specific_JJ idiosyncrasies_NNS ._. Since_IN all_DT of_IN training_NN articles_NNS came_VBD from_IN the_DT Reuters_NNP news-wire_NN stories_NNS ,_, the_DT articles_NNS always_RB contain_VBP the_DT token_NN (_-LRB- Reuters_NNP )_-RRB- ,_, where_WRB “Reuters_NNS ”_NN is_VBZ flagged_VBN as_IN a_DT name_NN in_IN the_DT training_NN set_NN ._. The_DT classifier_NN learns_VBZ to_TO tag_VB candidate_NN tokens_NNS with_IN parentheses_NNS on_IN either_DT side_NN as_IN names_NNS ._. Finally_RB ,_, it_PRP is_VBZ promising_VBG that_IN although_IN there_EX are_VBP many_JJ input_NN features_NNS ,_, the_DT system_NN automatically_RB ignores_VBZ the_DT irrelevant_JJ features_NNS ._. When_WRB porting_VBG the_DT system_NN to_TO other_JJ languages_NNS or_CC genres_NNS ,_, it_PRP may_MD not_RB be_VB obvious_JJ which_WDT features_VBZ to_TO use_VB ._. The_DT system_NN allows_VBZ us_PRP to_TO add_VB many_JJ potential_JJ features_NNS from_IN which_WDT the_DT relevant_JJ ones_NNS are_VBP automatically_RB selected_VBN ._. As_IN mentioned_VBN earlier_RBR ,_, the_DT variability_NN in_IN language_NN makes_VBZ the_DT number_NN of_IN syntactic_JJ patterns_NNS that_IN the_DT system_NN has_VBZ to_TO learn_VB be_VB much_RB larger_JJR than_IN can_MD be_VB learned_VBN using_VBG the_DT limited_JJ training_NN data_NNS that_IN we_PRP had_VBD 6_CD COMPUTATIONAL_NNP INTELLIGENCE_NNP Rule_NNP 39_CD :_: comma-1_JJ =_SYM +1_CD word-in-dict-2_CD =_SYM -1_NN initial-caps-2_NN =_SYM +1_FW noun-3_JJ =_SYM -1_NN ->_NN named_VBN -_: entity_NN [97.8_CD %_NN ]_SYM Rule_NNP 91_CD :_: unknown-pos-1_NN =_SYM -1_NN proper-noun-2_NN =_SYM +1_FW all-uppercase-2_. =_SYM +1_CD ->_NN named_VBN -_: entity_NN [97.2_CD %_NN ]_NNS Rule_NNP 47_CD :_: adjective-1_NN =_SYM -1_NN word-in-dict-2_NN =_SYM -1_NN initial-caps-2_NN =_SYM +1_FW verb-3_JJ =_SYM +1_CD ->_NN named_VBN -_: entity_NN [95.8_CD %_NN ]_SYM Rule_NNP 121_CD :_: proper-noun-2_NN =_SYM +1_FW word-in-dict-2_CD =_SYM -1_NN initial-caps-2_NN =_SYM +1_FW ->_: named_VBN -_: entity_NN [95.3_CD %_NN ]_SYM FIGURE_NNP 1_CD ._. Sample_NNP rules_NNS generated_VBN by_IN C4.5_NNP ,_, with_IN one_CD word_NN context_NN ._. For_IN example_NN ,_, given_VBN three_CD tokens_NNS in_IN order_NN ,_, token-1_NN token-2_NN token-3_NN ,_, the_DT context_NN in_IN this_DT case_NN are_VBP token-1_JJ and_CC token-3_JJ ;_: token-2_NN is_VBZ the_DT token_JJ under_IN consideration_NN ._. To_TO illustrate_VB ,_, Rule_NNP #_# 47_CD states_NNS that_IN if_IN the_DT preceding_VBG word_NN is_VBZ not_RB an_DT adjective_NN ,_, the_DT current_JJ token_NN is_VBZ not_RB in_IN the_DT dictionary_NN ,_, has_VBZ an_DT initial_JJ upper-case_JJ letter_NN ,_, and_CC the_DT following_JJ word_NN is_VBZ a_DT verb_NN ,_, the_DT token_JJ under_IN consideration_NN is_VBZ a_DT named-entity_NN (_-LRB- if_IN none_NN of_IN the_DT preceding_VBG rules_NNS apply_VBP )_-RRB- ._. −2.50_CD −2.00_CD −1.50_CD −1.00_CD −0.50_CD 0.00_CD 0.50_CD 1.00_CD 1.50_CD 2.00_CD 2.50_CD 3.00_CD 3.50_CD A_DT ll_NN Up_IN pe_NN rc_NN as_IN e_NN In_IN iti_DT al_JJ C_NNP ap_NN s_VBZ A_DT dje_NN cti_NN ve_JJ N_NNP ou_MD n_RB Pr_NNP op_NN er_NN N_NNP ou_MD n_RB A_DT lp_NN ha_WP nu_NN m_NN er_NN ic_JJ D_NNP et_NNP er_NNP m_NN in_IN er_NN W_NNP or_CC d_VBN in_IN D_NNP ic_JJ t_NN Si_NNP ng_VBG le_JJ C_NNP ha_NN r_NN ._. Si_'' ng_VBG le_JJ ’_NN I_PRP ’_VBP ’_VBN +_RB ’_RB o_IN r_NN ’_NN −_NN ’_, All_DT Features_NNS −_VBP 0_CD Context_NNP −1.00_CD −0.50_CD 0.00_CD 0.50_CD 1.00_CD 1.50_CD 2.00_CD 2.50_CD All_DT Features_NNP −_VBD 1_CD Word_NNP Context_NNP Le_NNP ft_VBD Pa_NNP re_VBD n_RB ._. Pe_DT rio_NN d_VBD A_DT lp_NN ha_WP nu_NN m_NN er_NN ic_JJ A_DT ll_NN Up_IN pe_NN rc_NN as_IN e_NN D_NNP et_NNP er_NNP m_NN in_IN er_NN A_DT dje_NN cti_NN ve_JJ N_NNP ou_MD n_RB Pr_NNP op_NN er_NN N_NNP ou_MD n_RB W_NNP or_CC d_VBN in_IN D_NNP ic_JJ t_NN A_DT ll_NN Up_IN pe_NN rc_NN as_IN e_NN Si_IN ng_VBG le_JJ C_NNP ha_NN r_NN ._. In_IN iti_FW al_JJ C_NNP ap_NN s_VBZ A_DT lp_NN ha_WP nu_NN m_NN er_NN ic_JJ Si_NNP ng_NN le_NN ’_IN I_PRP ’_VBP Si_NNP ng_VBG le_JJ ’S_NNS ’_RB +_RB o_IN r_NN −_NN In_IN iti_DT al_JJ C_NNP ap_NN s_VBZ R_NN ig_CC ht_NN P_NNP ar_NN en_FW ._. WORD_FW (_-LRB- N−1_NN )_-RRB- WORD_NN (_-LRB- N_NNP )_-RRB- WORD_NNP (_-LRB- N+1_NN )_-RRB- Le_FW ft_VBD Pa_NNP re_VBD n_RB ._. FIGURE_CD 2._CD Feature_NNP weightings_NNS when_WRB using_VBG all_DT knowledge_NN sources_NNS ;_: effects_NNS of_IN zero_CD and_CC one_CD word_NN of_IN context_NN before_IN and_CC after_IN the_DT word_NN to_TO be_VB classified_VBN are_VBP shown_VBN ._. Positive_NNP values_NNS signal_VBP a_DT name_NN ,_, and_CC negative_JJ values_NNS signal_NN against_IN a_DT name_NN ._. For_IN clarity_NN ,_, only_RB weights_NNS with_IN large_JJ magnitudes_NNS are_VBP labeled_VBN here_RB ._. available_JJ ._. To_TO increase_VB the_DT number_NN of_IN effective_JJ patterns_NNS that_WDT are_VBP learned_VBN ,_, we_PRP collapsed_VBD adjacent_JJ similar_JJ tokens_NNS into_IN a_DT single_JJ token_JJ ._. When_WRB using_VBG this_DT system_NN in_IN practice_NN (_-LRB- after_IN training_NN )_-RRB- ,_, adjacent_JJ tokens_NNS that_WDT were_VBD classified_VBN as_IN nouns_NNS by_IN the_DT POS_NNP tagger_NN were_VBD collapsed_VBN into_IN a_DT single_JJ token_JJ ._. Using_VBG such_JJ a_DT cascaded_JJ processing_NN model_NN resulted_VBD in_IN improved_JJ performance_NN ,_, with_IN the_DT system’s_NN performance_NN now_RB beginning_VBG APPLYING_NNP MACHINE_NNP LEARNING_NNP FOR_IN HIGH_NNP PERFORMANCE_NNP NAMED-ENTITY_NNP EXTRACTION_NNP 7_CD NEW_JJ YORK_NN (_-LRB- Reuters_NNP )_-RRB- -_: Hotel_NNP real_JJ estate_NN investment_NN trust_NN Patriot_NNP American_NNP Hospitality_NNP Inc._NNP said_VBD Tuesday_NNP it_PRP had_VBD agreed_VBN to_TO acquire_VB Interstate_NNP Hotels_NNPS Corp_NNP ,_, a_DT hotel_NN management_NN company_NN ,_, in_IN a_DT cash_NN and_CC stock_NN transaction_NN valued_VBN at_IN $_$ 2.1_CD billion_CD ,_, including_VBG the_DT assumption_NN of_IN $_$ 785_CD million_CD of_IN Interstate_NNP debt_NN ._. Interstate’s_'' portfolio_NN includes_VBZ 40_CD owned_VBN hotels_NNS and_CC resorts_NNS ,_, primarily_RB upscale_JJ ,_, full-service_JJ facilities_NNS ,_, leases_NNS for_IN 90_CD hotels_NNS ,_, and_CC management-service_NN agreements_NNS for_IN 92_CD hotels_NNS ._. On_IN completion_NN of_IN the_DT Interstate_NNP deal_NN and_CC its_PRP$ pending_VBG acquisitions_NNS of_IN Wyndham_NNP Hotel_NNP Corp._NNP and_CC WHG_NNP Resorts_NNP and_CC Casinos_NNP Inc._NNP ,_, Patriot’s_NNP portfolio_NN will_MD consist_VB of_IN 455_CD owned_VBN ,_, leased_VBN ,_, managed_VBN ,_, franchised_VBN or_CC serviced_VBN properties_NNS with_IN about_IN 103,000_CD rooms_NNS ._. A_DT definitive_JJ agreement_NN between_IN Patriot_NNP and_CC Interstate_NNP values_NNS Interstate_NNP at_IN $_$ 37.50_CD per_IN share_NN ._. Patriot_NNP will_MD pay_VB cash_NN for_IN 40_CD percent_NN of_IN Interstate’s_NNP shares_NNS ,_, and_CC will_MD exchange_VB Patriot_NNP paired_VBN shares_NNS for_IN the_DT rest_NN ._. Paired_'' shares_NNS trade_VBP jointly_RB for_IN real_JJ estate_NN investment_NN trusts_NNS and_CC their_PRP$ paired_VBN operating_NN companies_NNS ._. Patriot_NN said_VBD it_PRP expects_VBZ the_DT transaction_NN to_TO be_VB about_IN 8_CD percent_NN accretive_NN to_TO its_PRP$ funds_NNS from_IN operations_NNS ._. It_PRP said_VBD the_DT agreement_NN had_VBD been_VBN approved_VBN by_IN the_DT boards_NNS of_IN Interstate_NNP and_CC Wyndham_NNP Patriot_NNP said_VBD it_PRP did_VBD not_RB expect_VB the_DT deal_NN to_TO delay_VB the_DT closing_NN if_IN its_PRP$ transaction_NN with_IN Wyndham_NNP ,_, which_WDT is_VBZ to_TO close_VB by_IN year-end_NN ._. FIGURE_RB 3._'' This_DT example_NN illustrates_VBZ the_DT performance_NN of_IN the_DT system_NN (_-LRB- with_IN a_DT context_NN of_IN one_CD )_-RRB- ._. Underlined_VBD words_NNS indicate_VBP names_NNS that_WDT were_VBD successfully_RB detected_VBN ;_: italicized_VBN words_NNS mark_NN tokens_NNS that_WDT were_VBD misclassified_VBN as_IN names_NNS ;_: and_CC bold_JJ words_NNS are_VBP names_NNS that_WDT were_VBD not_RB found_VBN ._. See_VB the_DT text_NN for_IN details_NNS ._. to_TO match_VB the_DT best_JJS reported_JJ F1_JJ scores_NNS ._. With_IN one-word_JJ context_NN ,_, the_DT cascaded_JJ approach_NN improved_VBD the_DT F1_JJ score_NN from_IN from_IN 93.2_CD to_TO 95.2_CD ._. This_DT experiment_NN is_VBZ the_DT first_JJ attempt_NN of_IN reducing_VBG the_DT number_NN of_IN patterns_NNS to_TO be_VB learned_VBN ;_: other_JJ methods_NNS for_IN syntactic_JJ pattern_NN compression_NN are_VBP being_VBG explored_VBN ._. The_DT preceding_VBG discussion_NN ,_, along_IN with_IN the_DT quantitative_JJ experimental_JJ results_NNS reported_VBD ,_, has_VBZ identi-_VBN fied_VBN some_DT of_IN the_DT benefits_NNS and_CC drawbacks_NNS of_IN our_PRP$ approach_NN ._. We_PRP can_MD also_RB gain_VB insight_NN into_IN the_DT system’s_NN performance_NN on_IN the_DT name_NN detection_NN task_NN by_IN examining_VBG specific_JJ failure_NN cases_NNS ._. Figure_NN 3_CD presents_VBZ the_DT system’s_JJ output_NN on_IN a_DT randomly_RB selected_VBN news_NN story_NN ._. The_DT underlined_JJ words_NNS are_VBP names_NNS that_WDT were_VBD suc_JJ -_: cessfully_RB detected_VBN ;_: italicized_VBN words_NNS mark_NN tokens_NNS that_WDT were_VBD misclassified_VBN as_IN names_NNS ;_: and_CC bold_JJ words_NNS are_VBP names_NNS that_WDT were_VBD missed_VBN ._. In_IN this_DT example_NN ,_, the_DT system_NN failed_VBD to_TO find_VB two_CD names_NNS (_-LRB- “Interstate”_NN and_CC “Patriot”_NN )_-RRB- ,_, and_CC misclassified_VBD the_DT word_NN “upscale_NN ”_NN as_IN a_DT name_NN ._. It_PRP should_MD be_VB noted_VBN that_IN the_DT word_NN “upscale_NN ”_NN does_VBZ not_RB appear_VB in_IN the_DT system’s_NN dictionary_NN ,_, while_IN both_DT of_IN the_DT names_NNS ,_, “Interstate”_NNP and_CC “Patriot_NNP ”_NNP do_VBP ._. It_PRP should_MD be_VB noted_VBN ,_, however_RB ,_, that_IN the_DT latter_JJ errors_NNS are_VBP not_RB fully_RB explained_VBN by_IN the_DT dictionary_JJ confusion_NN :_: al-_: though_IN the_DT words_NNS “Interstate”_. and_CC “Patriot_JJ ”_NNS occur_VBP multiple_JJ times_NNS in_IN this_DT document_NN ,_, they_PRP are_VBP correctly_RB tagged_VBN in_IN every_DT other_JJ instance_NN ._. In_IN these_DT instances_NNS ,_, the_DT evidence_NN gathered_VBD from_IN sources_NNS other_JJ than_IN the_DT dictionary_NN was_VBD enough_RB to_TO outweigh_VB the_DT dictionary_JJ confusion_NN ._. 6_CD ._. CONCLUSIONS_NNP AND_CC FUTURE_NNP WORK_NN This_DT paper_NN presents_VBZ high-performance_JJ name_NN spotting_NN based_VBN on_IN machine_NN learning_NN ._. Although_IN it_PRP achieves_VBZ performance_NN comparable_JJ to_TO the_DT best_JJS name_NN detection_NN systems_NNS ,_, it_PRP does_VBZ not_RB rely_VB on_IN hand-crafted_JJ rules_NNS nor_CC large_JJ manually_RB created_VBN lists_NNS of_IN names_NNS ._. This_DT paper_NN also_RB presents_VBZ an_DT analysis_NN of_IN different_JJ knowledge_NN sources_NNS ,_, combinations_NNS of_IN which_WDT can_MD yield_VB widely_RB varying_VBG results_NNS ._. To_TO design_VB larger_JJR systems_NNS which_WDT address_VBP more_RBR complex_JJ tasks_NNS ,_, it_PRP is_VBZ important_JJ to_TO determine_VB which_WDT knowledge_NN sources_NNS provide_VBP the_DT best_JJS discrimination_NN power_NN ,_, and_CC which_WDT are_VBP redundant_NN ._. In_IN future_JJ work_NN ,_, we_PRP plan_VBP to_TO extend_VB this_DT system_NN to_TO other_JJ tasks_NNS ,_, as_RB well_RB as_IN complete_JJ the_DT MUC_NNP task_NN 8_CD COMPUTATIONAL_NNP INTELLIGENCE_NN specifications_NNS ,_, which_WDT include_VBP name-spotting_NN and_CC categorizing_VBG them_PRP as_IN people-names_NNS ,_, location-names_NNS and_CC organization-names_NNS ._. Once_RB a_DT potential_JJ name_NN has_VBZ been_VBN identified_VBN ,_, there_EX are_VBP several_JJ cues_NNS that_WDT can_MD be_VB exploited_VBN to_TO determine_VB to_TO which_WDT of_IN these_DT three_CD categories_NNS it_PRP belongs_VBZ ._. Additionally_RB ,_, we_PRP plan_VBP to_TO explore_VB hybrid_JJ systems_NNS where_WRB our_PRP$ approach_NN is_VBZ used_VBN in_IN conjunction_NN with_IN traditional_JJ parsing_NN techniques_NNS ._. REFERENCES_NNS J._NNP Aberdeen_NNP ,_, J._NNP Burger_NNP ,_, David_NNP Day_NNP ,_, Lynette_NNP Hirschman_NNP ,_, P._NNP Robinson_NNP ,_, and_CC Marc_NNP Vilain_NNP ._. MITRE_NNP :_: Description_NN of_IN the_DT alembic_JJ system_NN used_VBN for_IN MUC-6_NN ._. In_IN Proceedings_NNP of_IN the_DT Sixth_NNP Message_NN Understanding_NN Conference_NNP (_-LRB- MUC-6_NN )_-RRB- ,_, pages_NNS 141–155_CD ,_, Columbia_NNP ,_, MD_NNP ,_, 1995_CD ._. NIST_NNP ,_, Morgan-Kaufmann_NNP Publishers_NNPS ._. D._NNP Appelt_NNP ,_, J._NNP Hobbs_NNP ,_, D._NNP Israel_NNP ,_, and_CC M._NNP Tyson_NNP ._. FASTUS_NNP :_: A_DT finite-state_NN processor_NN for_IN information_NN extraction_NN from_IN real-world_NN text_NN ._. In_IN Proceedings_NNP IJCAI-93_NNP ,_, 1993_CD ._. T._NNP Appelt_NNP ,_, J._NNP Hobbs_NNP ,_, J._NNP Bear_NNP ,_, D._NNP Israel_NNP ,_, M._NNP Kameyama_NNP ,_, A._NN Kehler_NNP ,_, D._NNP Martin_NNP ,_, K._NNP Myers_NNP ,_, and_CC M._NNP Tyson_NNP ._. SRI_NNP international_JJ FASTUS_NNP system_NN MUC-6_CC test_NN results_NNS and_CC analysis_NN ._. In_IN Proceedings_NNP of_IN the_DT Sixth_NNP Message_NN Un-_NNP derstanding_NN Conference_NNP (_-LRB- MUC-6_NN )_-RRB- ,_, Columbia_NNP ,_, MD_NNP ,_, 1995_CD ._. NIST_NNP ,_, Morgan-Kaufmann_NNP Publishers_NNPS ._. D._NNP Bikel_NNP ,_, S._NNP Miller_NNP ,_, R._NNP Schwartz_NNP ,_, and_CC R._NNP Weischedel_NNP ._. NYMBLE_NNP :_: a_DT high-performance_NN learning_VBG name-finder_NN ._. In_IN Proceedings_NNP of_IN the_DT Fifth_NNP Conference_NNP on_IN Applied_NNP Natural_NNP Language_NNP Processing_NNP ,_, pages_NNS 194–201_CD ,_, Washington_NNP ,_, D.C._NNP ,_, 1997_CD ._. ACL._NNP A._NNP Borkovsky_NNP ._. Knight-Ridder_NNP Information’s_NNP Value_NNP Adding_NNP Name_NNP Finder_NNP ._. A_DT Variation_NNP on_IN the_DT Theme_NNP of_IN Fastus_NNP ._. In_IN Proceedings_NNP of_IN the_DT Sixth_NNP Message_NN Understanding_NN Conference_NNP (_-LRB- MUC-6_NN )_-RRB- ,_, Columbia_NNP ,_, MD_NNP ,_, 1995_CD ._. NIST_NNP ,_, Morgan-Kaufmann_NNP Publishers_NNPS ._. Eric_NNP Brill_NNP ._. Some_DT advances_NNS in_IN rule_NN based_VBN part-of-speech_NN tagging_NN ._. In_IN Proceedings_NNP of_IN the_DT Twelfth_NNP National_NNP Confer-_NNP ence_NN on_IN Artificial_NNP Intelligence_NNP ,_, pages_NNS 722–727_CD ,_, Seattle_NNP ,_, WA_NNP ,_, 1994_CD ._. AAAI._NNP Eric_NNP Brill_NNP ._. Transformation-based_'' error-driven_JJ learning_NN and_CC natural_JJ language_NN processing_NN :_: A_DT case_NN study_NN in_IN part_NN of_IN speech_NN tagging_VBG ._. Computational_NNP Linguistics_NNP ,_, 21(_CD 4)_CD :543–566_NNS ,_, December_NNP 1995_CD ._. R._NNP Gaizauskas_NNP ,_, T._NNP Wakao_NNP ,_, K._NNP Humphreys_NNP ,_, H._NNP Cunningham_NNP ,_, and_CC Y_NNP ._. Wilks_VBZ ._. University_NNP of_IN Sheffield_NNP :_: Description_NN of_IN the_DT LaSIE_NNP system_NN as_IN used_VBN for_IN MUC-6_NN ._. In_IN Proceedings_NNP of_IN the_DT Sixth_NNP Message_NN Understanding_NN Conference_NNP (_-LRB- MUC-6_NN )_-RRB- ,_, Columbia_NNP ,_, MD_NNP ,_, 1995_CD ._. NIST_NNP ,_, Morgan-Kaufmann_NNP Publishers_NNPS ._. R._NNP Grishman_NNP and_CC B._NNP Sundheim_NNP ._. Design_NNP of_IN the_DT MUC-6_NNP evaluation_NN ._. In_IN Proceedings_NNP of_IN the_DT Sixth_NNP Message_NN Under-_NNP standing_NN Conference_NNP (_-LRB- MUC-6_NN )_-RRB- ,_, Columbia_NNP ,_, MD_NNP ,_, 1995_CD ._. NIST_NNP ,_, Morgan-Kaufmann_NNP Publishers_NNPS ._. L._NNP Iwanska_NNP ,_, M._NNP Croll_NNP ,_, T._NNP Yoon_NNP ,_, and_CC M._NNP Adams_NNP ._. Wayne_NNP state_NN university_NN :_: Description_NN of_IN the_DT UNO_NNP natural_JJ language_NN processing_NN system_NN as_IN used_VBN for_IN MUC-6_NN ._. In_IN Proceedings_NNP of_IN the_DT Sixth_NNP Message_NN Understanding_NN Conference_NNP (_-LRB- MUC-6_NN )_-RRB- ,_, Columbia_NNP ,_, MD_NNP ,_, 1995_CD ._. NIST_NNP ,_, Morgan-Kaufmann_NNP Publishers_NNPS ._. R._NNP Morgan_NNP ,_, R._NNP Garigliano_NNP ,_, P._NNP Callaghan_NNP ,_, S._NNP Poria_NNP ,_, M._NNP Smith_NNP ,_, A._NN Urbanowicz_NNP ,_, R._NNP Collingham_NNP ,_, M._NNP Costantino_NNP ,_, C._NNP Cooper_NNP ,_, and_CC the_DT LOLITA_NNP Group_NNP ._. University_NNP of_IN durham_NNP :_: Description_NN of_IN the_DT LOLITA_NNP system_NN as_IN used_VBN for_IN MUC-6_NN ._. In_IN Proceedings_NNP of_IN the_DT Sixth_NNP Message_NN Understanding_NN Conference_NNP (_-LRB- MUC-6_NN )_-RRB- ,_, Columbia_NNP ,_, MD_NNP ,_, 1995_CD ._. NIST_NNP ,_, Morgan-Kaufmann_NNP Publishers_NNPS ._. David_NNP D._NNP Palmer_NNP and_CC David_NNP S._NNP Day_NNP ._. A_DT statistical_JJ profile_NN of_IN the_DT Named-Entity_NN Task_NNP ._. In_IN Proceedings_NNP of_IN the_DT Fifth_NNP Conference_NNP on_IN Applied_NNP Natural_NNP Language_NNP Processing_NNP ,_, pages_NNS 190–193_CD ,_, Washington_NNP ,_, D.C._NNP ,_, 1997_CD ._. ACL._NNP David_NNP D._NNP Palmer_NNP and_CC Marti_NNP A._NNP Hearst_NNP ._. Adaptive_NNP sentence_NN boundary_NN disambiguation_NN ._. In_IN Proceedings_NNP of_IN the_DT 1994_CD Conference_NNP on_IN Applied_NNP Natural_NNP Language_NNP Processing_NNP ,_, Stuttgart_NNP ,_, Germany_NNP ,_, October_NNP 1994_CD ._. ACL._NNP J._NNP Ross_NNP Quinlan_NNP ._. C4.5_'' :_: Programs_NNS for_IN Machine_NN Learning_NNP ._. Morgan_NNP Kaufmann_NNP Series_NNP in_IN Machine_NNP Learning_NNP ._. Morgan-Kaufmann_NNP Publishers_NNPS ,_, Menlo_NNP Park_NNP ,_, CA_NNP ,_, 1992_CD ._. Lawrence_NNP Rabiner_NNP ._. A_DT tutorial_NN on_IN hidden_JJ markov_NN models_NNS and_CC selective_JJ applications_NNS in_IN speech_NN recognition_NN ._. In_IN Alex_NNP Waibel_NNP and_CC K._NNP F._NNP Lee_NNP ,_, editors_NNS ,_, Readings_NNS in_IN Speech_NNP Recognition_NNP ._. Morgan_NNP Kaufmann_NNP Publishers_NNPS ,_, 1993_CD ._. Jeffrey_NNP C._NNP Reynar_NNP and_CC Adwait_NNP Ratnaparkhi_NNP ._. A_DT maximum_NN entropy_NN approach_NN to_TO identifying_VBG sentence_NN boundaries_VBZ ._. In_IN Proceedings_NNP of_IN the_DT Fifth_NNP Conference_NNP on_IN Applied_NNP Natural_NNP Language_NNP Processing_NNP ,_, pages_NNS 16–19_CD ,_, Washington_NNP ,_, D.C._NNP ,_, 1997_CD ._. ACL._NNP B._NNP Sundheim_NNP ._. Overview_NNP of_IN results_NNS of_IN the_DT MUC-6_NNP evaluation_NN ._. In_IN Proceedings_NNP of_IN the_DT Sixth_NNP Message_NN Understanding_NN Conference_NNP (_-LRB- MUC-6_NN )_-RRB- ,_, Columbia_NNP ,_, MD_NNP ,_, 1995_CD ._. NIST_NNP ,_, Morgan-Kaufmann_NNP Publishers_NNPS ._. APPLYING_NNP MACHINE_NNP LEARNING_NNP FOR_IN HIGH_NNP PERFORMANCE_NNP NAMED-ENTITY_NNP EXTRACTION_NNP 9_CD Paul_NNP Thompson_NNP and_CC Christopher_NNP C._NNP Dozier_NNP ._. Name_VB searching_VBG and_CC information_NN retrieval_NN ._. Available_JJ in_IN the_DT The_NNP Computation_NNP and_CC Language_NNP E-Print_NNP Archive_NNP at_IN http_NN :_: //xxx_NN .lanl.gov/abs/cmp_NN −_IN lg/9706017/_NN ,_, June_NNP 1997_CD ._. C._NNP J._NNP van_NNP Rijsbergen_NNP ._. Information_NNP Retrieval_NNP ._. Butterworths_NNPS ,_, London_NNP ,_, 1979_CD ._. Ralph_NNP Weischedel_NNP ._. BBN_NNP :_: description_NN of_IN the_DT PLUM_NNP system_NN as_IN used_VBN for_IN MUC-6_NN ._. In_IN Proceedings_NNP of_IN the_DT Sixth_NNP Message_NN Understanding_NN Conference_NNP (_-LRB- MUC-6_NN )_-RRB- ,_, Columbia_NNP ,_, MD_NNP ,_, 1995_CD ._. NIST_NNP ,_, Morgan-Kaufmann_NNP Publishers_NNPS ._.
